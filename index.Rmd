---
title: "Portfolio Computational Musicology"
author: "Ben Wöltgens"
date: "March 2024"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    theme:
      heading_font:
        google: 
          family: Rajdhani
          wght: 700
      base_font:
        google: Fira Sans
      code_font:
        google: Fira Mono
      bg: "#FFFFFF"
      fg: "#212529" 
      primary: "#0d114a"
      secondary: "#39d7b8"
      success: "#39d7b8"
      danger: "#fa5577"
      warning: "#ffb14c"
      info: "#0cc7f1"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)
library(ggplot2)
library(flexdashboard)
library(compmus)
library(spotifyr)
library(cowplot)
library(plotly)
```

```{r include=FALSE}
playlist <- get_playlist_audio_features(playlist_uris = c("558qxWvGDmCG2y2w3OZpcG"))
Clipping <- get_artist_audio_features('5HJ2kX5UTwN4Ns8fB5Rn1I')
Jpegmafia <- get_artist_audio_features('6yJ6QQ3Y5l0s0tn7b0arrO')
Death_Grips <- get_artist_audio_features('5RADpgYLOuS2ZxDq7ggYYH')
Scorn <- get_artist_audio_features('0sKlAr34dAjAdb7a9yriqX')
YBT <- get_artist_audio_features('4JGRL8a7h6Cv7hNqbkyloW')
```




### Introduction

This portfolio aims to analyze a specific branch of hip-hop, namely industrial hip-hop. Industrial hip-hop, in my opinion, is one of the most interesting and bizarre genres in rap music. Being a fusion of industrial music and hip-hop, the genre showcases artists blending harsh, mechanical, transgressive or provocative sounds accompanied with rap and rhythm. Due to being a niche genre, one which many people may not be familiar with, I believe it would be a very interesting topic to explore in this online portfolio. The aim of the portfolio is to research how industrial hip-hop formed as a unique branch in hip-hop and, more specifically, to analyze songs in the genre from the 80’s to the current day to see what elements they have in common, and in which way these songs differ and have evolved. Elements such as loudness, lyrics, timbre, and bpm would be interesting to explore over time as I believe there are visible changes in these areas throughout the years, such as loudness and bpm increasing. The portfolio would possibly also include comparing industrial hip-hop to other branches of rap music in order to better understand what elements in a song make it uniquely industrial and set it apart from its counterparts.

Artists I wish to analyze for this portfolio include the following:

2000’s-present: Jpegmafia, Clipping, Death Grips. These are all artists I personally listen to and all bring their own unique sound to the industrial hip-hop genre.

1990’s: Young Black Tennagers, Scorn. These I believe can show more about the forming of more experimentation with industrial noise in rap music.

1980’s: Mark Stewart, Bill Laswell, and Adrian Sherwood. These artists appear to have set the roots for industrial hip-hop, and so analyzing these songs I believe will reveal more about the forming of the genre itself.

### First visualization

```{r first_plot, echo=FALSE}
present_data = rbind(Clipping, Jpegmafia, Death_Grips)
p1 <- ggplot(playlist, aes(x=energy, y=loudness, color = valence, size = track.popularity, label = track.name )) + 
geom_point() + ggtitle("Industrial hip-hop artists' songs measured by Loudness and Energy") + theme(plot.title = element_text(size = 10)) + labs(x = "Energy", y="Loudness", color = "Valence")

p2 <- ggplot(playlist, aes(x=energy, y=valence, color = ar, label = track_name )) + 
geom_point() + ggtitle("Current industrial hip-hop artists' songs measured by Valence and Energy") + theme(plot.title = element_text(size = 10)) + labs(x = "Energy", y="Valence", color = "Artists")

ggplotly(p1)
```

------------------------------------------------------------------------

This visualization shows the relations between loudness and energy in Industrial music. The tracks were taken from a industrial hip-hop playlist. Size is determined by the track's popularity while color is determined by the track's valence. This plot shows a clear correlation between loudness and energy among these artist, namely the louder the songs, the more energetic they are. With this we can get a better understanding of what makes a hip-hop song industrial. In terms of valence and popularity there doesn't seem to be an obvious pattern.

### Chromagrams

```{r echo=FALSE}
isf <-
  get_tidy_audio_analysis("7nCONy10IHp7XD3oYZ0lcx") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

aintitfunny <-
  get_tidy_audio_analysis("4tU1vbxn9rvUv9VuAUXERx") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

scorn <-
  get_tidy_audio_analysis("6imWTAoXsDuWypXlHJtErI") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

```

```{r pop-chroma-plots, echo=FALSE}

isf_plot <- 
  isf |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  geom_vline(xintercept = 163, colour = "#e3e3e3") +
  geom_vline(xintercept = 182, colour = "#e3e3e3") +
  labs(
    x = "Time (s)", 
    y = NULL, 
    fill = "Magnitude", 
    title = "I've Seen Footage (Death Grips)"
  ) +
  theme_minimal() +
  scale_fill_viridis_c() 


aintitfunny_plot <- 
  aintitfunny |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  geom_vline(xintercept = 184, colour = "#e3e3e3") +
  labs(
    x = "Time (s)", 
    y = NULL, 
    fill = "Magnitude", 
    title = "Ain't it Funny (Danny Brown)"
  ) +
  theme_minimal() +
  scale_fill_viridis_c() 


scorn_plot <- 
  scorn |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  geom_vline(xintercept = 45, colour = "#e3e3e3") +
  geom_vline(xintercept = 194, colour = "#e3e3e3") +
  geom_vline(xintercept = 202, colour = "#e3e3e3") +
  geom_vline(xintercept = 427, colour = "#e3e3e3") +
  labs(
    x = "Time (s)", 
    y = NULL, 
    fill = "Magnitude", 
    title = "Silver Rain Fell (Scorn)"
  ) +
  theme_minimal() +
  scale_fill_viridis_c() 

plot_grid(isf_plot, aintitfunny_plot, scorn_plot, ncol = 1)
```

------------------------------------------------------------------------

The first plot shows the chromagram of "I've seen Footage" by Death Grips. The somewhat noisy nature of industrial hip-hop makes it very difficult for spotify to recognize and distinguish certain chords. This is very apparent in this song as there are very few major contrasts in magnitude. Most of the song seems to quite consistently switches between C and F#, with a slight change between 163-182s. The switch between C and F# is also known as a tritone, or the devil's interval. These intervals are extremely dissonant yet they frequently appear in other Industrial hip-hop tracks.

The second plot analyses "Ain't it Funny" by Danny brown, which switches intervals between G and C#. G is used during the multiple choruses, while C# is used for the many verses where the artist then raps. This again is another False Tritone, similar to "I've seen Footage".

The third chromagram analyses "Silver Rain Fell" by Scorn, an artist from the 90's which has had an influence on the development of Industrial music and possibly Industrial Hip-Hop. This track mainly switches between G/Gb and E/Eb, along with C#. The lines represent changes in the main verse, which seem to be very minor. 


### Self-similarity Matrix


```{r ssm, echo=FALSE}
bzt <-
  get_tidy_audio_analysis("7nCONy10IHp7XD3oYZ0lcx") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  bzt |> 
    compmus_self_similarity(pitches, "aitchison") |> 
    mutate(d = d / max(d), type = "Chroma"),
  bzt |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() + 
  labs(x = "", y = "")
```

------------------------------------------------------------------------

This shows the self-similarity matrices of the song "I've Seen Footage" by Death Grips. This song is structurally quite interesting since we don't really see the typical chorus-verse structure throughout the entire song. The song builds upon the chorus-verse structure with 3 brief sections at the start, visualised by the three small squares up until around the 30s mark. From there song takes a more familiar form up until around 120s when it calls back to a section from the intro. Afterwards we get a longer than usual verse followed by a bridge into the chorus.

### Key and Chord estimations


```{r keychord, echo=FALSE}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

hazard_duty_pay <-
  get_tidy_audio_analysis("5dLz8bhINeCWgppiUIcafp") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

hdp_keys <-
hazard_duty_pay |> 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  mutate(d=1 - d) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(option="inferno", guide = "none") +
  theme_minimal() +
  labs(title = "HAZARD DUTY PAY! (JPEGMAFIA)", x = "Time (s)", y = "") +
  theme(plot.title = element_text(hjust = 0.5, size=10))

hdp_chords <-
hazard_duty_pay |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  mutate(d=1 - d) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(option="inferno", guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")

say_the_name <-
  get_tidy_audio_analysis("3YHhWdTlWjIML155CvcH4F") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

stn_keys <-
say_the_name |> 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  mutate(d=1 - d) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(option="inferno", guide = "none") +
  theme_minimal() +
  labs(title = "Say the name (clipping)", x = "Time (s)", y = "") +
  theme(plot.title = element_text(hjust = 0.5, size=10))

stn_chords <-
say_the_name |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  mutate(d=1 - d) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(option="inferno", guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```


```{r keychord_plot, echo=FALSE}
plot_grid(hdp_keys,stn_keys , hdp_chords, stn_chords, ncol = 2, labels = c("Keys", "Keys", "Chords", "Chords"), label_x = c(0,0,-0.1,-0.1), label_y = c(1,1,1.1,1.1))
```

------------------------------------------------------------------------

This page shows the keygram and chordgrams of two songs: HAZARD DUTY PAY! by JPEGMAFIA, and Say the Name by clipping. These keygrams and chordgrams show the matching of each key/chord per time frame, with yellow being more often matched and blue less so.

With JPEGMAFIA's track we can see that spotify cannot identify which keys are used in the track, resulting in a completely yellow keygram. The chordgram shows that the song seems to consist only of 7th chords, but appearances can be deceiving.
We used the euclidean distance metric to match chords to points in the songs.
However, Spotify seems to think that every pitch class is active through the
entire song. Seventh chords will always match more strongly than chord triads
in this case, simply because they consist of more notes.

Clipping's keys seem to be in C# mainly, but the mode is rather ambiguous. Its chord progression seems to chiefly consist of the I, II and VI chords, 
I think that I -> II -> VI progression is most likely.

### Tempograms

```{r echo=FALSE}
#Hazard duty pay: 5dLz8bhINeCWgppiUIcafp
# Have a sad cum BB
hscb <- get_tidy_audio_analysis("6X2oT45uXeP6mnqN4EEj7y")
```

```{r echo=FALSE}
tempo1 <-
hscb |>
  tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) |> #Change hop-size if it's too slow
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()
```


```{r echo=FALSE}
tempo2 <-
hscb |>
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()
```

```{r echo=FALSE}
plot_grid(tempo1, tempo2, nrow=2)
```

------------------------------------------------------------------------

This page shows the tempograms for the song "Have a sad ### BB" by Death grips.`I believe this song would be interesting to analyze seeing as it use some rather bizarre mixing and sampling throughout the entire track, and to see whether Spotify could then pick up on the tempo. The measured tempo in the top visualization at first glance looks incredibly messy, though a keen eye might see that there does seem to be a couple of vague lines at around 300BPM and 600BPM. The perceived tempo in the bottom plot also look chaotic, however there is a much clearer tempo visible in the plot. The perceived tempo seems to be somewhere between 143-155BPM, though this seems to fluctuate a bit in Spotify's analysis.

### Conclusion/discussion

To be assessed. 
