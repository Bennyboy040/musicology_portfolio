---
title: "Portfolio Computational Musicology"
author: "Ben WÃ¶ltgens"
date: "March 2024"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    theme:
      heading_font:
        google: 
          family: Rajdhani
          wght: 700
      base_font:
        google: Fira Sans
      code_font:
        google: Fira Mono
      bg: "#FFFFFF"
      fg: "#212529" 
      primary: "#0d114a"
      secondary: "#39d7b8"
      success: "#39d7b8"
      danger: "#fa5577"
      warning: "#ffb14c"
      info: "#0cc7f1"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(knitr)
library(ggplot2)
library(flexdashboard)
library(compmus)
library(spotifyr)
library(cowplot)
library(plotly)
library(ggdendro)
library(heatmaply)
library(ranger)

get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit |> 
    collect_predictions() |> 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit |> 
    conf_mat_resampled() |> 
    group_by(Prediction) |> mutate(precision = Freq / sum(Freq)) |> 
    group_by(Truth) |> mutate(recall = Freq / sum(Freq)) |> 
    ungroup() |> filter(Prediction == Truth) |> 
    select(class = Prediction, precision, recall)
}  
```

```{r include=FALSE}
playlist <- get_playlist_audio_features(playlist_uris = c("558qxWvGDmCG2y2w3OZpcG"))
Clipping <- get_artist_audio_features('5HJ2kX5UTwN4Ns8fB5Rn1I')
Jpegmafia <- get_artist_audio_features('6yJ6QQ3Y5l0s0tn7b0arrO')
Death_Grips <- get_artist_audio_features('5RADpgYLOuS2ZxDq7ggYYH')
Scorn <- get_artist_audio_features('0sKlAr34dAjAdb7a9yriqX')
YBT <- get_artist_audio_features('4JGRL8a7h6Cv7hNqbkyloW')
```



### Introduction

This portfolio aims to analyze a specific branch of Hip-Hop, namely industrial hip-hop. Industrial Hip-Hop, in my opinion, is one of the most interesting and bizarre genres in rap music. Being a fusion of Industrial music and Hip-Hop, the genre showcases artists blending harsh, mechanical, transgressive or provocative sounds accompanied with rap and rhythm. Due to it being a niche genre, one which many people may not be familiar with, I believe it would be a very interesting topic to explore in this online portfolio. The goal of this portfolio is to research how Spotify interprets and understands Industrial Hip-Hop by analyzing features and songs in the genre to see how this branch differs from other Hip-Hop genres.

My corpus consists of the public playlist titled Noise/Industrial hip-hop, which contains 36 songs of the genre. Along with the playlist I also included a few distinct tracks from artists such as Jpegmafia, Clipping and Death Grips, as they are likely among the most well-known artists in the Industrial Hip-Hop genre.

------------------------------------------------------------------------

<iframe src="https://open.spotify.com/embed/playlist/558qxWvGDmCG2y2w3OZpcG?si=0558c62e937d4c6b?utm_source=generator" width="100%" height="380" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

### How energetic and loud is industrial hip-hop?

```{r first_plot, echo=FALSE}
present_data = rbind(Clipping, Jpegmafia, Death_Grips)
p1 <- ggplot(playlist, aes(x=energy, y=loudness, color = valence, size = track.popularity, label = track.name )) + 
geom_point() + ggtitle("Industrial hip-hop artists' songs measured by Loudness and Energy") + theme(plot.title = element_text(size = 10)) + labs(x = "Energy", y="Loudness", color = "Valence")

p2 <- ggplot(playlist, aes(x=energy, y=valence, color = ar, label = track_name )) + 
geom_point() + ggtitle("Current industrial hip-hop artists' songs measured by Valence and Energy") + theme(plot.title = element_text(size = 10)) + labs(x = "Energy", y="Valence", color = "Artists")

ggplotly(p1)
```

------------------------------------------------------------------------

This visualization shows the relations between loudness and energy in Industrial music. The tracks were taken from a industrial hip-hop playlist. Size is determined by the track's popularity while color is determined by the track's valence. This plot shows a clear correlation between loudness and energy among these artist, namely the louder the songs, the more energetic they are. With this we can get a better understanding of what makes a hip-hop song industrial. In terms of valence and popularity there doesn't seem to be an obvious pattern.

### A couple of chromagrams

```{r echo=FALSE}
isf <-
  get_tidy_audio_analysis("7nCONy10IHp7XD3oYZ0lcx") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

aintitfunny <-
  get_tidy_audio_analysis("4tU1vbxn9rvUv9VuAUXERx") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

scorn <-
  get_tidy_audio_analysis("6imWTAoXsDuWypXlHJtErI") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

```

```{r pop-chroma-plots, echo=FALSE}

isf_plot <- 
  isf |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  geom_vline(xintercept = 163, colour = "#e3e3e3") +
  geom_vline(xintercept = 182, colour = "#e3e3e3") +
  labs(
    x = "Time (s)", 
    y = NULL, 
    fill = "Magnitude", 
    title = "I've Seen Footage (Death Grips)"
  ) +
  theme_minimal() +
  scale_fill_viridis_c() 


aintitfunny_plot <- 
  aintitfunny |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  geom_vline(xintercept = 184, colour = "#e3e3e3") +
  labs(
    x = "Time (s)", 
    y = NULL, 
    fill = "Magnitude", 
    title = "Ain't it Funny (Danny Brown)"
  ) +
  theme_minimal() +
  scale_fill_viridis_c() 


scorn_plot <- 
  scorn |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  geom_vline(xintercept = 45, colour = "#e3e3e3") +
  geom_vline(xintercept = 194, colour = "#e3e3e3") +
  geom_vline(xintercept = 202, colour = "#e3e3e3") +
  geom_vline(xintercept = 427, colour = "#e3e3e3") +
  labs(
    x = "Time (s)", 
    y = NULL, 
    fill = "Magnitude", 
    title = "Silver Rain Fell (Scorn)"
  ) +
  theme_minimal() +
  scale_fill_viridis_c() 

plot_grid(isf_plot, aintitfunny_plot, scorn_plot, ncol = 1)
```

------------------------------------------------------------------------

The first plot shows the chromagram of "I've seen Footage" by Death Grips. The somewhat noisy nature of industrial hip-hop makes it very difficult for spotify to recognize and distinguish certain chords. This is very apparent in this song as there are very few major contrasts in magnitude. Most of the song seems to quite consistently switches between C and F#, with a slight change between 163-182s. The switch between C and F# is also known as a tritone, or the devil's interval. These intervals are extremely dissonant yet they frequently appear in other Industrial hip-hop tracks.

The second plot analyses "Ain't it Funny" by Danny brown, which switches intervals between G and C#. G is used during the multiple choruses, while C# is used for the many verses where the artist then raps. This again is another False Tritone, similar to "I've seen Footage".

The third chromagram analyses "Silver Rain Fell" by Scorn, an artist from the 90's which has had an influence on the development of Industrial music and possibly Industrial Hip-Hop. This track mainly switches between G/Gb and E/Eb, along with C#. The lines represent changes in the main verse, which seem to be very minor. 


### What is the structure of 'I've seen footage?'


```{r ssm, echo=FALSE}
bzt <-
  get_tidy_audio_analysis("7nCONy10IHp7XD3oYZ0lcx") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  bzt |> 
    compmus_self_similarity(pitches, "aitchison") |> 
    mutate(d = d / max(d), type = "Chroma"),
  bzt |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() + 
  labs(x = "", y = "")
```

------------------------------------------------------------------------

This shows the self-similarity matrices of the song "I've Seen Footage" by Death Grips. This song is structurally quite interesting since we don't really see the typical chorus-verse structure throughout the entire song. The song builds upon the chorus-verse structure with 3 brief sections at the start, visualised by the three small squares up until around the 30s mark. From there song takes a more familiar form up until around 120s when it calls back to a section from the intro. Afterwards we get a longer than usual verse followed by a bridge into the chorus.

### Taking a look at keys and chords?


```{r keychord, echo=FALSE}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

hazard_duty_pay <-
  get_tidy_audio_analysis("5dLz8bhINeCWgppiUIcafp") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

hdp_keys <-
hazard_duty_pay |> 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  mutate(d=1 - d) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(option="inferno", guide = "none") +
  theme_minimal() +
  labs(title = "HAZARD DUTY PAY! (JPEGMAFIA)", x = "Time (s)", y = "") +
  theme(plot.title = element_text(hjust = 0.5, size=10))

hdp_chords <-
hazard_duty_pay |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  mutate(d=1 - d) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(option="inferno", guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")

say_the_name <-
  get_tidy_audio_analysis("3YHhWdTlWjIML155CvcH4F") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

stn_keys <-
say_the_name |> 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  mutate(d=1 - d) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(option="inferno", guide = "none") +
  theme_minimal() +
  labs(title = "Say the name (clipping)", x = "Time (s)", y = "") +
  theme(plot.title = element_text(hjust = 0.5, size=10))

stn_chords <-
say_the_name |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  mutate(d=1 - d) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(option="inferno", guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```


```{r keychord_plot, echo=FALSE}
plot_grid(hdp_keys,stn_keys , hdp_chords, stn_chords, ncol = 2, labels = c("Keys", "Keys", "Chords", "Chords"), label_x = c(0,0,-0.1,-0.1), label_y = c(1,1,1.1,1.1))
```

------------------------------------------------------------------------

This page shows the keygram and chordgrams of two songs: HAZARD DUTY PAY! by JPEGMAFIA, and Say the Name by clipping. These keygrams and chordgrams show the matching of each key/chord per time frame, with yellow being more often matched and blue less so.

With JPEGMAFIA's track we can see that spotify cannot identify which keys are used in the track, resulting in a completely yellow keygram. The chordgram shows that the song seems to consist only of 7th chords, but appearances can be deceiving.
We used the euclidean distance metric to match chords to points in the songs.
However, Spotify seems to think that every pitch class is active through the
entire song. Seventh chords will always match more strongly than chord triads
in this case, simply because they consist of more notes.

Clipping's keys seem to be in C# mainly, but the mode is rather ambiguous. Its chord progression seems to chiefly consist of the I, II and VI chords, 
I think that I -> II -> VI progression is most likely.

### Struggling to find the beat

```{r echo=FALSE}
#Hazard duty pay: 5dLz8bhINeCWgppiUIcafp
# Have a sad cum BB
hscb <- get_tidy_audio_analysis("6X2oT45uXeP6mnqN4EEj7y")
```

```{r echo=FALSE}
tempo1 <-
hscb |>
  tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) |> #Change hop-size if it's too slow
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()
```


```{r echo=FALSE}
tempo2 <-
hscb |>
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()
```

```{r echo=FALSE}
plot_grid(tempo1, tempo2, nrow=2)
```

------------------------------------------------------------------------

This page shows the tempograms for the song "Have a sad ### BB" by Death grips.`I believe this song would be interesting to analyze seeing as it use some rather bizarre mixing and sampling throughout the entire track, and to see whether Spotify could then pick up on the tempo. The measured tempo in the top visualization at first glance looks incredibly messy, though a keen eye might see that there does seem to be a couple of vague lines at around 300BPM and 600BPM. The perceived tempo in the bottom plot also look chaotic, however there is a much clearer tempo visible in the plot. The perceived tempo seems to be somewhere between 143-155BPM, though this seems to fluctuate a bit in Spotify's analysis.


### Analyzing feature importance using Random Forest

```{r echo=FALSE}
playlist2 <- get_playlist_audio_features(playlist_uris = c("6fOKSrN7vzwrMIeYFtvvhh"))
```

```{r echo=FALSE}
indie <-
  bind_rows(
    playlist |> mutate(playlist = "Industrial hip-hop") |> slice_head(n = 20),
    playlist2 |> mutate(playlist = "Industrial hip-hop2") |> slice_head(n = 20)
    ) |>
    add_audio_analysis()
```

```{r echo=FALSE}
indie_features <-
  indie |>  # For your portfolio, change this to the name of your corpus.
  mutate(
    playlist = factor(playlist),
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(
        segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean",
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))
```

```{r echo=FALSE}
indie_recipe <-
  recipe(
    playlist ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration +
      C + `C#|Db` + D + `D#|Eb` +
      E + `F` + `F#|Gb` + G +
      `G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 +
      c07 + c08 + c09 + c10 + c11 + c12,
    data = indie_features           # Use the same name as the previous block.
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors())      # Converts to z-scores.
  # step_range(all_predictors())    # Sets range to [0, 1].
```


```{r echo=FALSE}
indie_cv <- indie_features |> vfold_cv(5)
```

```{r echo=FALSE}
forest_model <-
  rand_forest() |>
  set_mode("classification") |> 
  set_engine("ranger", importance = "impurity")
indie_forest <- 
  workflow() |> 
  add_recipe(indie_recipe) |> 
  add_model(forest_model) |> 
  fit_resamples(
    indie_cv, 
    control = control_resamples(save_pred = TRUE)
  )
```

```{r echo=FALSE}
r1 <- workflow() |> 
  add_recipe(indie_recipe) |> 
  add_model(forest_model) |> 
  fit(indie_features) |> 
  pluck("fit", "fit", "fit") |>
  ranger::importance() |> 
  enframe() |> 
  mutate(name = fct_reorder(name, value)) |> 
  ggplot(aes(name, value)) + 
  geom_col() + 
  coord_flip() +
  theme_minimal() +
  labs(x = NULL, y = "Importance")
```

```{r echo=FALSE}
r2 <- indie_features |>
  ggplot(aes(x = speechiness, y = c08, size = energy)) +
  geom_point(alpha = 0.8) +
  scale_color_viridis_d() +
  labs(
    x = "Speechiness",
    y = "Timbre component 8",
    size = "Energy"
  )
```


```{r echo=FALSE}
plot_grid(r1,r2, nrow=1,rel_widths = c(1, 2))
```


------------------------------------------------------------------------

Using a random forest model, I decided to measure the importance of each feature in an industrial hip-hop track. The results showed that energy and speechiness ranked among the most important features, along with timbre component 8 and the G key. One would expect energy and speechiness to be important features in any hip-hop track, though the G keys' importance along with component 8 are quite interesting. Upon plotting these features, there seemed to be a slight correlation between component 8 and speechiness, with a high level of speechiness resulting in a higher level of component 8.


### Conclusion/discussion

To be assessed. 
